# Select 、poll、epoll

处理大量连接的读写，Apache所采用的select网络I/0模型比较低效。下面用一个比喻来解析Apache采用的select模型和Nginx采用的epol
1模型之间的区别:
假设你在大学读书，住的宿舍楼有很多间房间，你的朋友要来找你。select版宿管大妈就会带着你的朋友挨个房间去找，直到找到你为止。而epoll版宿管大妈会先记下每位进入同学的房间号，你的朋友来时，只需告诉你的朋友你住在哪个房间即可，不用亲自带着你的朋友满宿舍楼找人。如果来了10000 个人，都要找自己住这栋楼的问字的，Select版和epoll版宿管大妈，谁的效率更高，大家应该清楚了。同理，在高并发服务器中，轮询I/0是最耗时间的操作之一，select和epoll的谁的性能更高，同样十分明了。

![image-20210106162032927](https://picgo-w.oss-cn-chengdu.aliyuncs.com/img/image-20210106162032927.png)

![image-20210106162052831](https://picgo-w.oss-cn-chengdu.aliyuncs.com/img/image-20210106162052831.png)





由于IO操作涉及到系统调用，涉及到用户空间和内核空间的切换，所以理解系统的IO模型，对于需要进入到系统调用层面进行编程来说是很重要的。

## 阻塞IO和非阻塞IO

从程序编写的角度来看，I/O就是调用一个或多个系统函数，完成对输入输出设备的操作。输入输出设置可以是显示器、字符终端命令行、网络适配器、磁盘等。操作系统在这些设备与用户程序之间完成一个衔接，称为驱动程序，驱动程序向下驱动硬件，向上提供抽象的函数调用入口。

一般来说I/O操作是需要时间的，因为这涉及到系统、硬件等计算器模块的互相配合，所以必然不像普通的函数调用那样能够按照既定的方式立即返回。从用户代码的角度，I/O操作的系统调用分为“阻塞”和“非阻塞”两种。

- “阻塞”的调用会在I/O调用完成前，挂起调用线程，即CPU会不再执行后续代码，而是等到I/O完成后再回来继续执行，在用户代码看来，线程停止执行了，在调用处等待了。
- “非阻塞”的调用则不同，I/O调用基本上是立即返回，而且往往实际上I/O此时并没有完成，所以需要用户的程序轮询结果。

那么我们以网络IO为例，看一下对于一个服务器，“阻塞”和“非阻塞”两种模式，该如何设计。由于服务器要同时服务多个客户端，所以需要同时操作多个Socket。

![hello](https://picgo-w.oss-cn-chengdu.aliyuncs.com/img/hello.png)

可以看到，如果使用阻塞的IO方式，因为每个Socket都会阻塞，为了同时服务多个客户端，需要多个线程同时挂起；而如果采用非阻塞的调用方式，则需要在一个线程中不断轮训每个客户端是否有数据到来。

显然纯粹阻塞式的调用不可取，非阻塞式的调用看起来不错，但是仍不够好，因为轮询实际也是通过某种系统调用完成的，相当于在用户空间进行的，效率不高，如果能够在内核空间进行这种类似轮询，然后让内核通知用户空间哪个IO就绪了，就更好了。于是引出接下来的概念：`IO多路复用`

## IO多路复用

IO多路复用是一种系统调用，内核能够同时对多个IO描述符进行就绪检查。当所有被监听的IO都没有就绪时，调用将阻塞；当至少有一个IO描述符就绪时，调用将返回，用户代码可通过检查究竟是哪个IO就绪来进一步处理业务。显然，IO多路复用是解决系统里面存在N个IO描述符的问题的，这里必须明确IO复用和IO阻塞并不是一个概念，IO复用只检测IO是否就绪（读就绪或者写就绪等），具体的数据的输入输出还是需要依靠具体的IO操作完成（阻塞操作或非阻塞操作）。最典型的IO多路复用技术有`select`、`poll`、`epoll`等。`select`具有最大数量描述符限制，而`epoll`则没有，并且在机制上，`epoll`也更为高效。`select`的优势仅仅是跨平台支持性，所有平台和较低版本的内核都支持`select`模式，`epoll`则不是。

在IO相关的编程中，IO复用起到的作用相当于一个阀门，让后续IO操作更为精准高效。

## 编程模型

综上讨论，我们在进行实际的Socket编程的时候，无论是客户端还是服务端，大致有几种模式可以选择：

1. 阻塞式。纯采用阻塞式，这种方式很少见，基本只会出现在demo中。多个描述符需要用多个进程或者线程来一一对应处理。
2. 非阻塞式。纯非阻塞式，对IO的就绪与否需要在用户空间通过轮询来实现。
3. IO多路复用+阻塞式。仅使用一个线程就可以实现对多个描述符的状态管理，但由于IO输入输出调用本身是阻塞的，可能出现某个IO输入输出过慢，影响其他描述符的效率，从而体现出整体性能不高。此种方式编程难度比较低。
4. IO多路复用+非阻塞式。在多路复用的基础上，IO采用非阻塞式，可以大大降低单个描述符的IO速度对其他IO的影响，不过此种方式编程难度较高，主要表现在需要考虑一些慢速读写时的边界情况，比如读黏包、写缓冲不够等。

下面以select为例，整理 在select下，socket的阻塞和非阻塞的一些问题。这些细节在编写基于Socket的网络程序时，尤其是底层数据收发时，是十分重要的。

socket读就绪：

- 【阻/非阻】接收缓冲区有数据，数据量大于`SO_RCVLOWAT`水位（默认是0）。此时调用`recv`将返回>0(即读到的字节数)。
- 【阻/非阻】对端关闭，即收到FIN。此时调用`recv`将返回=0。
- 【阻/非阻】`accept`到一个新的连接，此时accept通常不会阻塞。
- 【阻/非阻】socket发生某种错误。此时调用recv将返回-1，并应通过`getsockopt`得到相应的待处理错误。

socket写就绪：

- 【阻/非阻】发送缓冲区有空余的空间，空间大小大于`SO_SNDLOWAT`水位（默认是2048）。这种就绪是水平触发的，只要有空间就会触发写就绪，即如果保持对这种套接字的就绪检查将使得`select`每次都认为有描述符写就绪。所以应当对描述符进行写状态管理，一旦某个描述符可写，应立即停止对该描述符的写状态检查，直到写缓冲区满后，再次select写状态。
- 【阻/非阻】连接的写半部关闭，此时调用send将产生`SIGPIPE`信号。
- 【非阻】`connect`完成。由于非阻的connect将不会阻塞握手过程，所以，当握手在后续时刻完成后，在此保持写状态检查，将触发一次就绪，表示connect完成。
- 【阻/非阻】socket发生某种错误。此时调用`send`将返回-1，并应通过`getsockopt`得到相应的待处理错误。

补充：

非阻的调用`recv`、`send`、`accept`，分别地，如果收缓冲中无数据、发送缓冲不够空间发、没有外来连接，将立即返回，此时全局`errno`将得到`EWOULDBLOCK`或`EAGIAN`，表示“本应阻塞的调用，由于采用了非阻塞模式，而返回”。非阻的调用`connect`将立即返回，此时全局`errno`将得到`EINPROGRESS`，表示连接正在进行。 

> I/O多路复用（multiplexing）的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作

select、poll 和 epoll 都是 Linux API 提供的 IO 复用方式。

相信大家都了解了Unix四种IO模型，

```
copy[1] blocking IO - 阻塞IO[2] nonblocking IO - 非阻塞IO[3] IO multiplexing - IO多路复用[4] asynchronous IO - 异步IO
```

在介绍select、poll、epoll之前，首先介绍一下Linux操作系统中基础的概念：

- 用户空间 / 内核空间
  现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。
  操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。
- 进程切换
  为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的，并且进程切换是非常耗费资源的。
- 进程阻塞
  正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得了CPU资源），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。
- 文件描述符
  文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。
  文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。
- 缓存I/O
  缓存I/O又称为标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存中，即数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

#### Select

我们先分析一下select函数

```
copyint select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout);
```

**【参数说明】**
**int maxfdp1**
指定待测试的文件描述字个数，它的值是待测试的最大描述字加1。
**fd_set**
fd_set可以理解为一个集合，这个集合中存放的是文件描述符(file descriptor)，即文件句柄。中间的三个参数指定我们要让内核测试读、写和异常条件的文件描述符集合。如果对某一个的条件不感兴趣，就可以把它设为空指针。
**timeout**
timeout告知内核等待所指定文件描述符集合中的任何一个就绪可花多少时间。其timeval结构用于指定这段时间的秒数和微秒数。

**【返回值】**
int 若有就绪描述符返回其数目，若超时则为0，若出错则为-1

##### select运行机制

select()的机制中提供一种fd_set的数据结构，实际上是一个long类型的数组，每一个数组元素都能与一打开的文件句柄（不管是Socket句柄,还是其他文件或命名管道或设备句柄）建立联系，建立联系的工作由程序员完成，当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读。

从流程上来看，使用select函数进行IO请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。但是，使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

##### select机制的问题

1. 每次调用select，都需要把fd_set集合从用户态拷贝到内核态，如果fd_set集合很大时，那这个开销也很大
2. 同时每次调用select都需要在内核遍历传递进来的所有fd_set，如果fd_set集合很大时，那这个开销也很大
3. 为了减少数据拷贝带来的性能损坏，内核对被监控的fd_set集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)

#### Poll

poll的机制与select类似，与select在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。也就是说，poll只解决了上面的问题3，并没有解决问题1，2的性能开销问题。

下面是pll的函数原型：

```
copyint poll(struct pollfd *fds, nfds_t nfds, int timeout);typedef struct pollfd {        int fd;                         // 需要被检测或选择的文件描述符        short events;                   // 对文件描述符fd上感兴趣的事件        short revents;                  // 文件描述符fd上当前实际发生的事件} pollfd_t;
```

poll改变了文件描述符集合的描述方式，使用了pollfd结构而不是select的fd_set结构，使得poll支持的文件描述符集合限制远大于select的1024

**【参数说明】**

**fds**
fds是一个struct pollfd类型的数组，用于存放需要检测其状态的socket描述符，并且调用poll函数之后fds数组不会被清空；一个pollfd结构体表示一个被监视的文件描述符，通过传递fds指示 poll() 监视多个文件描述符。其中，结构体的events域是监视该文件描述符的事件掩码，由用户来设置这个域，结构体的revents域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域

**nfds**
记录数组fds中描述符的总数量

**【返回值】**
int 函数返回fds集合中就绪的读、写，或出错的描述符数量，返回0表示超时，返回-1表示出错；

#### Epoll

epoll在Linux2.6内核正式提出，是基于事件驱动的I/O方式，相对于select来说，epoll没有描述符个数限制，使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

Linux中提供的epoll相关函数如下：

```
copyint epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

1.**epoll_create** 函数创建一个epoll句柄，参数size表明内核要监听的描述符数量。调用成功时返回一个epoll句柄描述符，失败时返回-1。

2.**epoll_ctl** 函数注册要监听的事件类型。四个参数解释如下：

- **epfd** 表示epoll句柄
- **op** 表示fd操作类型，有如下3种
  - EPOLL_CTL_ADD 注册新的fd到epfd中
  - EPOLL_CTL_MOD 修改已注册的fd的监听事件
  - EPOLL_CTL_DEL 从epfd中删除一个fd
- **fd** 是要监听的描述符
- **event** 表示要监听的事件

epoll_event 结构体定义如下：

```
copystruct epoll_event {    __uint32_t events;  /* Epoll events */    epoll_data_t data;  /* User data variable */};typedef union epoll_data {    void *ptr;    int fd;    __uint32_t u32;    __uint64_t u64;} epoll_data_t;
```

3.epoll_wait 函数等待事件的就绪，成功时返回就绪的事件数目，调用失败时返回 -1，等待超时返回 0。

- **epfd** 是epoll句柄
- **events** 表示从内核得到的就绪事件集合
- **maxevents** 告诉内核events的大小
- **timeout** 表示等待的超时事件

epoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。

epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

- **水平触发（LT）**：默认工作模式，即当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用epoll_wait时，会再次通知此事件
- **边缘触发（ET）**： 当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）。

LT和ET原本应该是用于脉冲信号的，可能用它来解释更加形象。Level和Edge指的就是触发点，Level为只要处于水平，那么就一直触发，而Edge则为上升沿和下降沿的时候触发。比如：0->1 就是Edge，1->1 就是Level。

ET模式很大程度上减少了epoll事件的触发次数，因此效率比LT模式下高。

总结
一张图总结一下select,poll,epoll的区别：
![select-poll-epoll](https://picgo-w.oss-cn-chengdu.aliyuncs.com/img/1586596177.png)

epoll是Linux目前大规模网络并发程序开发的首选模型。在绝大多数情况下性能远超select和poll。目前流行的高性能web服务器Nginx正式依赖于epoll提供的高效网络套接字轮询服务。但是，在并发连接不高的情况下，多线程+阻塞I/O方式可能性能更好。